{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_ipython().run_line_magic('matplotlib', 'inline')\n", "import os\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import statsmodels.api as sm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Test Poisson regression - Liverpool"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## read data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_df_team(team, season_start='1011', season_end='1718'):\n", "    df_seasons = []\n", "    \n", "    nseason = int(season_end[:2]) - int(season_start[:2]) + 1\n", "    \n", "    for iseason in range(nseason):\n", "        #season = f'{1011 + 101*iseason}'\n", "        season = f'{int(season_start[:2])+iseason:02d}{int(season_start[2:])+iseason:02d}'\n", "        try:\n", "            df_iseason = pd.read_csv(f'team_data/{season}/{team}.csv')\n", "            df_iseason.insert(0, 'Season', season)\n", "            df_seasons.append(df_iseason)\n", "            #df_seasons.append(pd.read_csv(f'team_data/{season}/{team}.csv'))\n", "        except FileNotFoundError:\n", "            continue\n", "        \n", "    if df_seasons == []:\n", "        raise ValueError(\"Empty (Wrong name of team?)\")\n", "    else:\n", "        df_seasons = pd.concat(df_seasons)\n", "        df_seasons = df_seasons.reset_index(drop=True)\n", "        return df_seasons"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = get_df_team('Liverpool')\n", "print(df.loc[0, 'Date'], 'to', df.loc[len(df)-1, 'Date'])\n", "print('df.shape =', df.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.columns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df['Goal']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cols = [\n", "    'Round', 'isHome', \n", "    'b5MatchGoal', \n", "    'bRival5MatchGoal', 'bRival5MatchConceded',\n", "    'SelfAS', 'SelfDS', 'RivalAS', 'RivalDS',\n", "    'SelfFromCL', 'RivalFromCL'\n", "]\n", "X = df[cols].copy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["['PtDiff'] = df['bCumPoints'] - df['bRivalCumPoints']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X['PtDiff'] = df['bStdCumPoints'] - df['bRivalStdCumPoints']\n", "#X['PtDiff_Round'] = X['Round'] * X['PtDiff']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X['isDec'] = pd.DatetimeIndex(df['Date']).month == 12"]}, {"cell_type": "markdown", "metadata": {}, "source": ["convert all bool-columns into numerical"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boolcol = X.dtypes == bool\n", "X.loc[:,boolcol] = X.loc[:,boolcol] + 0"]}, {"cell_type": "markdown", "metadata": {}, "source": [" = sm.add_constant(X)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X.insert(0, 'const', 1)\n", "X.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## fit model with all variables"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["res = sm.GLM(y, X, family=sm.families.Poisson(), missing='drop').fit()\n", "print('AIC = ', res.aic)\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["res.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## find the best model by backward selection"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BackwardElimination:\n", "    def __init__(self, model, y, X, model_kwargs=None, columns=None):\n", "        self.model = model\n", "        self.y = y\n", "        self.X = X\n", "        self.model_kwargs = model_kwargs\n", "        self.columns = columns\n", "        \n", "    def find(self, print_iter=False):\n", "        \"\"\"return the variables and the minimal AIC\"\"\"\n", "        model = self.model\n", "        y = self.y\n", "        X = self.X\n", "        model_kwargs = self.model_kwargs\n", "        \n", "        candidates = set(range(1, X.shape[1]))   # start from 1 to exclude intercept term\n", "        \n", "        # fit full model: `target_aic` should be as lower as possible\n", "        target_aic = model(y, X, **model_kwargs).fit().aic\n", "        \n", "        iiter = 1\n", "        while True:\n", "            cs = []\n", "            aics = []\n", "            for c in candidates:\n", "                aic = self._drop(c, candidates)\n", "                aics.append(aic)\n", "                cs.append(c)\n", "                \n", "            minidx = np.array(aics).argmin()\n", "            \n", "            if print_iter:\n", "                self._print_iteration(iiter, candidates, aics, minidx, target_aic)\n", "            \n", "            if aics[minidx] <= target_aic:\n", "                target_aic = aics[minidx]\n", "                candidates.remove(cs[minidx])\n", "                iiter += 1\n", "            else:\n", "                break\n", "                \n", "        if self.columns is None:\n", "            # add intercept term back\n", "            return [0] + list(candidates), target_aic\n", "        else:\n", "            return self.columns[[0] + list(candidates)], target_aic\n", "    \n", "    def _drop(self, c, candidates):\n", "        \"\"\"drop `c` in `candidates` and return the AIC\"\"\"\n", "        model = self.model\n", "        y = self.y\n", "        X = self.X\n", "        model_kwargs = self.model_kwargs\n", "        \n", "        dropped_cand = [0] + list(candidates.difference({c}))\n", "        return model(y, X[:,dropped_cand], **model_kwargs).fit().aic\n", "    \n", "    def _print_iteration(self, iiter, candidates, aics, minidx, target_aic):\n", "        print('=======================================')\n", "        print('Iteration: ', iiter)\n", "        \n", "        if self.columns is None:\n", "            print('minimal dropped variable: ', minidx)\n", "        else:\n", "            print('minimal dropped variable: ', self.columns[list(candidates)[minidx]])\n", "            \n", "        print('corresponding AIC: ', aics[minidx])\n", "        print('original AIC: ', target_aic)\n", "        if aics[minidx] <= target_aic:\n", "            print('drop or stop:  [Drop]')\n", "        else:\n", "            print('drop or stop:  [Stop]')\n", "            \n", "        print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idxcol, aic = BackwardElimination(\n", "    sm.GLM, \n", "    y.values, X.values, \n", "    model_kwargs={'family': sm.families.Poisson(), 'missing': 'drop'},\n", "    columns = X.columns\n", ").find(print_iter=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["idxcol, aic"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["res = sm.GLM(y, X[idxcol], family=sm.families.Poisson(), missing='drop').fit(scale='X2')\n", "res.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Pearson chi2 for this model:')\n", "res.pearson_chi2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## compare the Poisson regression model with `HomeAwayModel` and `PoiDistModel`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calc_pearson_chi2(predict, obs, var_func):\n", "    return np.sum((predict - obs) ** 2 / var_func(predict))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from simple_model.home_away_model import HomeAwayModel\n", "from simple_model.poidist_model import PoiDistModel"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["homeaway_res = []\n", "poidist_res = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i, row in df.iterrows():\n", "    season = row['Season']\n", "    team = 'Liverpool'\n", "    rival = row['Rival']\n", "    is_home = row['isHome']\n", "    \n", "    mu_ha = HomeAwayModel(season, team, is_home).mu\n", "    mu_pd = PoiDistModel(season, team, rival, is_home).mu\n", "    \n", "    homeaway_res.append(mu_ha)\n", "    poidist_res.append(mu_pd)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Pearson chi2 for HomeAwayModel and PoiDistModel:')\n", "print(calc_pearson_chi2(np.array(homeaway_res), y.values, lambda mu: mu))\n", "print(calc_pearson_chi2(np.array(poidist_res), y.values, lambda mu: mu))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Build models for all teams in 18/19 season"]}, {"cell_type": "markdown", "metadata": {}, "source": ["trainning data: 10/11 to 17/18 season"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["allteams = list(\n", "    map(\n", "        lambda s: s.split('.csv')[0],\n", "        filter(lambda s: s.endswith('.csv'), os.listdir('team_data/1819/'))\n", "    )\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dict[str_team_name, df]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["team_df_dict = {team: get_df_team(team) for team in allteams}\n", "team_df_dict.keys()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[14]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["team_X_dict = {}    # Dict[str_team_name, X]\n", "team_y_dict = {}    # Dict[str_team_name, y]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for team, df in team_df_dict.items():\n", "    y = df['Goal']\n", "    cols = [\n", "        'Round', 'isHome', \n", "        'b5MatchGoal', \n", "        'bRival5MatchGoal', 'bRival5MatchConceded',\n", "        'SelfAS', 'SelfDS', 'RivalAS', 'RivalDS',\n", "        'SelfFromCL', 'RivalFromCL'\n", "    ]\n", "    X = df[cols].copy()\n", "    X['PtDiff'] = df['bStdCumPoints'] - df['bRivalStdCumPoints']\n", "    #X['PtDiff_Round'] = X['Round'] * X['PtDiff']\n", "    X['isDec'] = pd.DatetimeIndex(df['Date']).month == 12\n\n", "    # convert all bool-columns into numerical\n", "    boolcol = X.dtypes == bool\n", "    X.loc[:,boolcol] = X.loc[:,boolcol] + 0\n\n", "    #X = sm.add_constant(X)\n", "    X.insert(0, 'const', 1)\n", "    \n", "    team_X_dict[team] = X\n", "    team_y_dict[team] = y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[15]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["team_res_dict = {}   # Dict[str_team_name, GLM_result_obj]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for team in allteams:\n", "    print(team, end=' / ')\n", "    \n", "    y = team_y_dict[team]\n", "    X = team_X_dict[team]\n", "    idxcol, aic = BackwardElimination(\n", "        sm.GLM, \n", "        y.values, X.values, \n", "        model_kwargs={'family': sm.families.Poisson(), 'missing': 'drop'},\n", "        columns = X.columns\n", "    ).find(print_iter=False)\n", "    \n", "    res = sm.GLM(y, X[idxcol], family=sm.families.Poisson(), missing='drop').fit(scale='X2')\n", "    team_res_dict[team] = res\n", "    \n", "print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## check the variables of each model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[16]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_regvar = pd.DataFrame(\n", "    index=team_res_dict.keys(),                # teams\n", "    columns=team_X_dict[allteams[0]].columns   # all possible variables\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for team, res in team_res_dict.items():\n", "    col = team_res_dict[team].params.index\n", "    #df_regvar.loc[team, res.model.exog_names] = True\n", "    df_regvar.loc[team, col] = team_res_dict[team].params\n", "    \n", "df_regvar"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## check the model fitness"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["the scale of pearson chi2 would be different because the number of data is different"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for team, res in team_res_dict.items():\n", "    print(f'{team:15} {res.pearson_chi2:7.3f} {res.pearson_chi2/res.nobs:7.3f}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# `PredictGoals`: using the previous models to predict the match results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.stats import norm, poisson"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PredictGoals:\n", "    def __init__(self, team_res_dict):\n", "        self.team_res_dict = team_res_dict\n", "        \n", "    def _read_team_data(self, season):\n", "        \"\"\"\n", "        return a dict: Dict[str_team_name, df_of_team]\n", "        the df is from /team_data/<season>/*.csv\n", "        \"\"\"\n", "        allteams = list(\n", "            map(\n", "                lambda s: s.replace('.csv', ''),\n", "                filter(\n", "                    lambda s: s.endswith('.csv'),\n", "                    os.listdir(f'team_data/{season}/')\n", "                )\n", "            )\n", "        )\n", "        \n", "        team_data_dict = {}\n", "        for team in allteams:\n", "            df = pd.read_csv(f'team_data/{season}/{team}.csv')\n", "            team_data_dict[team] = df\n", "            \n", "        return team_data_dict\n", "    \n", "    def _convert_to_reg_data(self, team_data_dict):\n", "        \"\"\"\n", "        Extract variables from team_data_dict, and convert to the variables used\n", "        in regression model.\n", "        Input: \n", "            team_data_dict is from self._read_team_data()\n", "        Return:\n", "            (reg_y_dict, reg_data_dict)\n", "            reg_y_dict    : Dict[str_team_name, goals]  \n", "            reg_data_dict : Dict[str_team_name, df_reg_data] \n", "            All of `df_reg_data` have the same columns.\n", "        \"\"\"\n", "        reg_y_dict = {}      \n", "        reg_data_dict = {}  \n", "        \n", "        cols = [\n", "            'Round', 'isHome', \n", "            'b5MatchGoal', \n", "            'bRival5MatchGoal', 'bRival5MatchConceded',\n", "            'SelfAS', 'SelfDS', 'RivalAS', 'RivalDS',\n", "            'SelfFromCL', 'RivalFromCL'\n", "        ]\n", "        \n", "        for team, df_team_data in team_data_dict.items():\n", "            X = df_team_data[cols].copy()\n", "            \n", "            X['PtDiff'] = df_team_data['bStdCumPoints'] - df_team_data['bRivalStdCumPoints']\n", "            X['PtDiff_Round'] = X['Round'] * X['PtDiff']\n", "            \n", "            X['isDec'] = pd.DatetimeIndex(df_team_data['Date']).month == 12\n", "            \n", "            # convert all bool-columns into numerical\n", "            boolcol = X.dtypes == bool\n", "            X.loc[:,boolcol] = X.loc[:,boolcol] + 0\n\n", "            #X = sm.add_constant(X)\n", "            X.insert(0, 'const', 1)\n", "            \n", "            reg_y_dict[team] = df_team_data['Goal'].values\n", "            reg_data_dict[team] = X\n", "            \n", "        return reg_y_dict, reg_data_dict\n", "    def _conf_int_mu(self, X, beta, cov_mat, inv_link_func, alpha=0.05):\n", "        \"\"\"\n", "        parameter\n", "            X.shape = (n, p)\n", "            beta.shape = (p,)\n", "            cov_mat.shape = (p, p)\n", "            inv_link_func : inverse link function\n", "            alpha : scalar between 0 ~ 1\n", "        return : \n", "            ci, ci.shape = (n, 2)\n", "            ci[:,0] is the lower bound and ci[:,1] for upper bound\n", "        \"\"\"\n", "        eta = X @ beta     # (n,)\n", "        \n", "        eta_stderr = []    # (n,)\n", "        for xi in X:\n", "            eta_stderr.append(np.sqrt(xi @ cov_mat @ xi[:,np.newaxis]))\n", "        eta_stderr = np.array(eta_stderr).ravel()\n", "        \n", "        z_cri = norm.ppf(1-alpha/2)\n", "        \n", "        ci_lower = inv_link_func(eta - z_cri * eta_stderr)\n", "        ci_upper = inv_link_func(eta + z_cri * eta_stderr)\n", "        ci = np.vstack((ci_lower, ci_upper)).T\n", "        return ci\n", "        \n", "    def predict_by_real_data(self, season, alpha=0.05, ci_adjust=False):\n", "        \"\"\" \n", "        e.g. season = '1920' . alpha is for ci\n", "        \"\"\"\n", "        team_res_dict = self.team_res_dict\n", "        \n", "        # Dict[str_team_name, df_of_team]\n", "        # the df is read from: /team_data/<season>/<team_name>.csv\n", "        team_data_dict = self._read_team_data(season)\n", "        \n", "        # Dict[str_team_name, Array_goals] and Dict[str_team_name, df_of_team_data]\n", "        reg_y_dict, reg_data_dict = self._convert_to_reg_data(team_data_dict)\n", "        \n", "        # Dict[str_team_name, df_predict_result]\n", "        predict_res = {}\n", "        \n", "        # predict\n", "        for team, reg_data in reg_data_dict.items():           \n", "            glmres = team_res_dict[team]\n", "            var_col = glmres.model.exog_names\n", "            \n", "            y = reg_y_dict[team]    # (nobs,)\n", "            X = reg_data_dict[team].loc[:,var_col]   # (nobs, nfeatures)\n", "            \n", "            mu = glmres.predict(X)   # (nobs,)\n", "            predict_goals = np.floor(mu)\n", "            ci = self._conf_int_mu(\n", "                X.values, \n", "                glmres.params.values, \n", "                glmres.cov_params().values, \n", "                glmres.model.family.link.inverse,\n", "                alpha\n", "            )\n", "            \n", "            predict_res[team] = pd.DataFrame({\n", "                'Rival': team_data_dict[team].loc[:,'Rival'].values,\n", "                'isHome': team_data_dict[team].loc[:,'isHome'].values,\n", "                'obs': y,\n", "                'mu': mu,\n", "                'ci': ci.tolist(),\n", "                'predict': predict_goals\n", "            })\n", "            \n", "        # merge the predict result of rivals\n", "        # adding columns: 'Rival_obs', 'Rival_mu', 'Rival_predict', 'is_win_obs', 'is_win_predict_sep'\n", "        for team in reg_data_dict.keys():\n", "            predict_res[team] = self._merge_rival_predict(predict_res, team)\n", "            \n", "        # adding columns: 'prob_win', 'prob_draw', 'prob_loss'\n", "        for team in reg_data_dict.keys():\n", "            predict_res[team] = self._calc_prob(predict_res, team)\n", "            \n", "        # adding column: 'is_win_predict_jnt'\n", "        for team in reg_data_dict.keys():\n", "            predict_res[team] = self._calc_is_win_predict_jnt(predict_res, team, ci_adjust)\n", "        \n", "        # change order of column\n", "        for team in reg_data_dict.keys():\n", "            new_col = [\n", "                'Rival', 'isHome',\n", "                'obs', 'Rival_obs', \n", "                'mu', 'ci', 'Rival_mu', 'Rival_ci',\n", "                'predict', 'Rival_predict',\n", "                'prob_win', 'prob_draw', 'prob_loss',\n", "                'is_win_obs', 'is_win_predict_sep', 'is_win_predict_jnt'\n", "            ]\n", "            predict_res[team] = predict_res[team][new_col]\n", "            predict_res[team]['Rival_obs'] = predict_res[team]['Rival_obs'].astype(int)\n", "        \n", "        return predict_res\n", "    \n", "    def _merge_rival_predict(self, predict_res, team):\n", "        \"\"\"predict_res is from self.predict_by_real_data\"\"\"\n", "        team_pred_res = predict_res[team].copy()\n", "        rivals = team_pred_res['Rival'].unique()\n", "        \n", "        team_pred_res['Rival_obs'] = np.nan\n", "        team_pred_res['Rival_mu'] = np.nan\n", "        team_pred_res['Rival_ci'] = np.nan\n", "        team_pred_res['Rival_predict'] = np.nan\n", "        \n", "        # merge rival goals\n", "        for rival in rivals:\n", "            rival_pred_res = predict_res[rival]\n", "            idx = rival_pred_res['Rival'] == team\n", "            rival_obs_goal = rival_pred_res.loc[idx, 'obs'].values\n", "            rival_mu_goal = rival_pred_res.loc[idx, 'mu'].values\n", "            rival_ci_goal = rival_pred_res.loc[idx, 'ci'].values\n", "            rival_pred_goal = rival_pred_res.loc[idx, 'predict'].values\n", "            \n", "            idx2 = team_pred_res['Rival'] == rival\n", "            team_pred_res.loc[idx2, 'Rival_obs'] = rival_obs_goal\n", "            team_pred_res.loc[idx2, 'Rival_mu'] = rival_mu_goal\n", "            team_pred_res.loc[idx2, 'Rival_ci'] = rival_ci_goal\n", "            team_pred_res.loc[idx2, 'Rival_predict'] = rival_pred_goal\n", "        \n", "        # compute the match result\n", "        team_pred_res['is_win_obs'] = np.nan\n", "        team_pred_res.loc[team_pred_res.obs > team_pred_res.Rival_obs, 'is_win_obs'] = 'Win'\n", "        team_pred_res.loc[team_pred_res.obs == team_pred_res.Rival_obs, 'is_win_obs'] = 'Draw'\n", "        team_pred_res.loc[team_pred_res.obs < team_pred_res.Rival_obs, 'is_win_obs'] = 'Loss'\n", "        \n", "        team_pred_res['is_win_predict_sep'] = np.nan\n", "        team_pred_res.loc[team_pred_res.predict > team_pred_res.Rival_predict, 'is_win_predict_sep'] = 'Win'\n", "        team_pred_res.loc[team_pred_res.predict == team_pred_res.Rival_predict, 'is_win_predict_sep'] = 'Draw'\n", "        team_pred_res.loc[team_pred_res.predict < team_pred_res.Rival_predict, 'is_win_predict_sep'] = 'Loss'\n", "        \n", "        return team_pred_res\n", "    \n", "    def _calc_prob(self, predict_res, team):\n", "        \"\"\"\n", "        Calculate win/draw/loss probability based on `predict` and `Rival_predict`.\n", "        `predict_res` is from self.predict_by_real_data\n", "        \"\"\"\n", "        team_pred_res = predict_res[team].copy()\n", "        \n", "        # mu: parameter of poisson distribution\n", "        mu_team = team_pred_res['mu'].values   # (nobs,)\n", "        mu_rival = team_pred_res['Rival_mu'].values   # (nobs,)\n", "        \n", "        # goals (row vector), size=15\n", "        k = np.arange(15)[np.newaxis,:]    \n", "        \n", "        # calculate the probability\n", "        prob = []    # List[List[float]]\n", "        \n", "        for mu_tm, mu_rvl in zip(mu_team, mu_rival):\n", "            joint_pmf = poisson.pmf(k, mu_tm) * poisson.pmf(k.T, mu_rvl)   # (15, 15) \n", "            prob_win = np.triu(joint_pmf, k=1).sum()     # upper part for win\n", "            prob_draw = np.diag(joint_pmf).sum()         # diagonal for draw\n", "            prob_loss = np.tril(joint_pmf, k=-1).sum()   # lower part for loss\n", "            prob.append([prob_win, prob_draw, prob_loss])\n", "            \n", "        prob_df = pd.DataFrame(prob, columns=['prob_win', 'prob_draw', 'prob_loss'])\n", "        team_pred_res = pd.concat([team_pred_res, prob_df], axis=1)\n", "        return team_pred_res\n", "    \n", "    def _calc_is_win_predict_jnt(self, predict_res, team, ci_adjust=False):\n", "        \"\"\"\n", "        Calculate `is_win_predict_jnt` column\n", "        It just simply find the max one among three columns: 'prob_win', 'prob_draw' and 'prob_loss'.\n", "        If ci_adjust is True, `is_win_predict_jnt` will be 'Draw' when the difference between `mu` and \n", "        `Rival_mu` are not sifnificant (based on columns `ci` and `Rival_ci`).\n", "        \"\"\"\n", "        def find_is_win(row):\n", "            maxprob = row.astype(float).idxmax()\n", "            if maxprob == 'prob_win':\n", "                return 'Win'\n", "            elif maxprob == 'prob_draw':\n", "                return 'Draw'\n", "            elif maxprob == 'prob_loss':\n", "                return 'Loss'\n", "        \n", "        team_pred_res = predict_res[team].copy()\n", "        is_win_predict_jnt = team_pred_res[['prob_win', 'prob_draw', 'prob_loss']].apply(find_is_win, axis=1)\n", "        team_pred_res['is_win_predict_jnt'] = is_win_predict_jnt\n", "        \n", "        if ci_adjust:\n", "            isoverlap = team_pred_res[['ci', 'Rival_ci']].apply(self._is_overlap, axis=1)\n", "            team_pred_res.loc[isoverlap, 'is_win_predict_jnt'] = 'Draw'\n", "        \n", "        return team_pred_res\n", "    \n", "    def _is_overlap(self, lists):\n", "        \"\"\"\n", "        lists = [list1, list2]\n", "        check is list1 and list2 are overlap\n", "        e.g. list1 = [0, 8], list2 = [5, 10]   ->   True\n", "             list1 = [0, 8], list2 = [9, 12]   ->   False\n", "        \"\"\"\n", "        list1, list2 = lists\n", "        if np.any(np.isnan(list1)) or np.any(np.isnan(list2)):\n", "            return False\n", "        if max(list1) >= max(list2):\n", "            return False if min(list1) >= max(list2) else True\n", "        else:\n", "            return False if min(list2) >= max(list1) else True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pred_res_dict = PredictGoals(team_res_dict).predict_by_real_data('1819', alpha=0.35, ci_adjust=True)\n", "pred_res_dict['Liverpool'].head(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Confuse Matrix"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[21]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ConfuseMatrix:\n", "    def __init__(self, pred_res_dict):\n", "        \"\"\"\n", "        e.g. \n", "        >>> pred_res_dict = PredictGoals(team_res_dict).predict_by_real_data('1819')\n", "        >>> confuse_matrix(pred_res_dict['Liverpool'])\n", "        \"\"\"\n", "        self.pred_res_dict = pred_res_dict\n", "        \n", "    def confuse_matrix_by_team(self, team, divide_n=False, use='is_win_predict_sep'):\n", "        \"\"\"\n", "        team : str, name of the team\n", "        divide_n : bool, False for counting and True for proportion\n", "        use : str, determine use 'is_win_predict_sep' or 'is_win_predict_jnt'\n", "        \"\"\"\n", "        team_pred_res = self.pred_res_dict[team].dropna(axis=0)\n", "        n = team_pred_res.shape[0]\n", "        is_win_obs = team_pred_res['is_win_obs']\n", "        is_win_pred = team_pred_res[use]\n\n", "        # ww: observaed win, predict win; wd: observed win, predict draw; ...\n", "        ww = ((is_win_obs == 'Win') & (is_win_pred == 'Win')).sum()\n", "        wd = ((is_win_obs == 'Win') & (is_win_pred == 'Draw')).sum()\n", "        wl = ((is_win_obs == 'Win') & (is_win_pred == 'Loss')).sum()\n", "        dw = ((is_win_obs == 'Draw') & (is_win_pred == 'Win')).sum()\n", "        dd = ((is_win_obs == 'Draw') & (is_win_pred == 'Draw')).sum()\n", "        dl = ((is_win_obs == 'Draw') & (is_win_pred == 'Loss')).sum()\n", "        lw = ((is_win_obs == 'Loss') & (is_win_pred == 'Win')).sum()\n", "        ld = ((is_win_obs == 'Loss') & (is_win_pred == 'Draw')).sum()\n", "        ll = ((is_win_obs == 'Loss') & (is_win_pred == 'Loss')).sum()\n", "        confuse_mat = np.array([\n", "            [ww, dw, lw],\n", "            [wd, dd, ld],\n", "            [wl, dl, ll]\n", "        ])\n", "        if divide_n:\n", "            confuse_mat = confuse_mat / n\n", "        df = pd.DataFrame(confuse_mat)\n", "        df.index = ['Win', 'Draw', 'Loss']\n", "        df.columns = ['Win', 'Draw', 'Loss']\n", "        df.index.name = 'Predict'\n", "        df.columns.name = 'Obs'\n", "        return df\n", "    \n", "    def confuse_matrix_by_allteam(self, divide_n=False, use='is_win_predict_sep'):\n", "        allres = []\n", "        for team, team_pred in pred_res_dict.items():\n", "            allres.append(team_pred)\n", "    \n", "        allres = pd.concat(allres)\n", "        allres = allres.loc[allres.isHome,:].drop('isHome', axis=1).dropna(axis=0).reset_index(drop=True)\n", "    \n", "        n = allres.shape[0]\n", "    \n", "        is_win_obs = allres['is_win_obs']\n", "        is_win_pred = allres[use]\n", "        \n", "        # ww: observaed win, predict win; wd: observed win, predict draw; ...\n", "        ww = ((is_win_obs == 'Win') & (is_win_pred == 'Win')).sum()\n", "        wd = ((is_win_obs == 'Win') & (is_win_pred == 'Draw')).sum()\n", "        wl = ((is_win_obs == 'Win') & (is_win_pred == 'Loss')).sum()\n", "        \n", "        dw = ((is_win_obs == 'Draw') & (is_win_pred == 'Win')).sum()\n", "        dd = ((is_win_obs == 'Draw') & (is_win_pred == 'Draw')).sum()\n", "        dl = ((is_win_obs == 'Draw') & (is_win_pred == 'Loss')).sum()\n", "        \n", "        lw = ((is_win_obs == 'Loss') & (is_win_pred == 'Win')).sum()\n", "        ld = ((is_win_obs == 'Loss') & (is_win_pred == 'Draw')).sum()\n", "        ll = ((is_win_obs == 'Loss') & (is_win_pred == 'Loss')).sum()\n", "        \n", "        confuse_mat = np.array([\n", "            [ww, dw, lw],\n", "            [wd, dd, ld],\n", "            [wl, dl, ll]\n", "        ])\n", "        if divide_n:\n", "            confuse_mat = confuse_mat / n\n", "        \n", "        df = pd.DataFrame(confuse_mat)\n", "        df.index = ['Win', 'Draw', 'Loss']\n", "        df.columns = ['Win', 'Draw', 'Loss']\n", "        df.index.name = 'Predict'\n", "        df.columns.name = 'Obs'\n", "    \n", "        return df\n", "    def indicator(self, confuse_mat, beta=1):\n", "        \"\"\"\n", "        confuse_mat is from self.confuse_matrix_by_allteam(divide_n=False) or\n", "        self.confuse_matrix_by_team(team, divide_n=False)\n", "        beta is for F1 measure (beta=1 for equal weight)\n", "        \"\"\"\n", "        n = confuse_mat.values.sum()\n", "        \n", "        if n <= 1:\n", "            raise ValueError(\"`divide_n` should be False when computing the confuse matrix\")\n", "            \n", "        accuracy = np.diag(confuse_mat.values).sum() / n\n", "        \n", "        precision_w = confuse_mat.loc['Win','Win'] / confuse_mat.loc['Win',:].values.sum()\n", "        precision_d = confuse_mat.loc['Draw','Draw'] / confuse_mat.loc['Draw',:].values.sum()\n", "        precision_l = confuse_mat.loc['Loss','Loss'] / confuse_mat.loc['Loss',:].values.sum()\n", "        \n", "        recall_w = confuse_mat.loc['Win','Win'] / confuse_mat.loc[:,'Win'].values.sum()\n", "        recall_d = confuse_mat.loc['Draw','Draw'] / confuse_mat.loc[:,'Draw'].values.sum()\n", "        recall_l = confuse_mat.loc['Loss','Loss'] / confuse_mat.loc[:,'Loss'].values.sum()\n", "        \n", "        f1_w = (1+beta**2) * precision_w * recall_w / ((beta**2 * precision_w) + recall_w)\n", "        f1_d = (1+beta**2) * precision_d * recall_d / ((beta**2 * precision_d) + recall_d)\n", "        f1_l = (1+beta**2) * precision_l * recall_l / ((beta**2 * precision_l) + recall_l)\n", "        \n", "        df = pd.DataFrame([\n", "            [accuracy],\n", "            [precision_w],\n", "            [precision_d],\n", "            [precision_l],\n", "            [recall_w],\n", "            [recall_d],\n", "            [recall_l],\n", "            [f1_w],\n", "            [f1_d],\n", "            [f1_l]\n", "        ])\n", "        df = pd.Series([accuracy, precision_w, precision_d, precision_l, recall_w, recall_d, recall_l, f1_w, f1_d, f1_l])\n", "        df.index = pd.MultiIndex.from_tuples([\n", "            ('Accuracy', '-'),\n", "            ('Precision', 'Win'),\n", "            ('Precision', 'Draw'),\n", "            ('Precision', 'Loss'),\n", "            ('Recall', 'Win'),\n", "            ('Recall', 'Draw'),\n", "            ('Recall', 'Loss'),\n", "            ('F1 Score', 'Win'),\n", "            ('F1 Score', 'Draw'),\n", "            ('F1 Score', 'Loss')\n", "        ])\n", "        \n", "        return df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## create a confuse matrix"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[22]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cm = ConfuseMatrix(pred_res_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('===== use is_win_predict_sep =====')\n", "print('----------------------------------')\n", "print('********* confuse matrix *********')\n", "print(cm.confuse_matrix_by_allteam(divide_n=True))\n", "print()\n", "print('*********** indicator ************')\n", "print(cm.indicator(cm.confuse_matrix_by_allteam(divide_n=False)))\n", "print()\n", "print()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('===== use is_win_predict_jnt =====')\n", "print('----------------------------------')\n", "print('********* confuse matrix *********')\n", "print(cm.confuse_matrix_by_allteam(divide_n=True, use='is_win_predict_jnt'))\n", "print()\n", "print('*********** indicator ************')\n", "print(cm.indicator(cm.confuse_matrix_by_allteam(divide_n=False, use='is_win_predict_jnt')))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## check the accuracy of each team"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[23]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with warnings.catch_warnings():\n", "    warnings.simplefilter(\"ignore\")   # it would raise a warning during the calculation of f1 score, but I don't care\n", "    allteams = pd.read_csv('table/1819.csv')['Team'].values   # sorted by rank\n", "    \n", "    titles = ['Predicted by expected values', 'Predicted by joint probability']\n", "    uses = ['is_win_predict_sep', 'is_win_predict_jnt']\n", "    \n", "    for title, use in zip(titles, uses):\n", "        cm = ConfuseMatrix(pred_res_dict)\n", "        accu = []\n", "        for team in allteams:\n", "            conf_mat = cm.confuse_matrix_by_team(team, use=use)\n", "            accu.append(\n", "                cm.indicator(conf_mat).loc[('Accuracy', '-')]\n", "            )\n", "        accu_all = cm.indicator(cm.confuse_matrix_by_allteam(use=use)).loc[('Accuracy', '-')]\n", "        plt.figure(figsize=(8, 6))\n", "        plt.barh(allteams[::-1], accu[::-1], alpha=0.9, zorder=5)\n", "        ylim = plt.ylim()\n", "        plt.vlines(x=accu_all, ymin=ylim[0], ymax=ylim[1], colors='gray', ls='dashed', lw=2.5, zorder=10)\n", "        plt.text(x=accu_all+0.01, y=10, s='overall\\naccuracy', fontsize=18, color='gray')\n", "        plt.ylim(*ylim)\n", "        plt.xlabel('Accuracy', fontsize=18)\n", "        plt.yticks(fontsize=12)\n", "        plt.title(title, fontsize=15)\n", "        plt.grid(alpha=0.3, zorder=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[24]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["check the sensitivity of alpha (used in ci_adjust)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy = []\n", "f1score = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = np.arange(0.05, 0.95+0.1, 0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for a in alpha:\n", "    print(a)\n", "    pred_res_dict = PredictGoals(team_res_dict).predict_by_real_data('1819', alpha=a, ci_adjust=True)\n", "    \n", "    cm = ConfuseMatrix(pred_res_dict)\n", "    confuse_mat = cm.confuse_matrix_by_allteam(divide_n=False, use='is_win_predict_jnt')\n", "    \n", "    indicator = cm.indicator(confuse_mat)\n", "    accu = indicator.loc[('Accuracy', '-')]\n", "    f1 = indicator.loc[('F1 Score', 'Draw')]\n", "    \n", "    accuracy.append(accu)\n", "    f1score.append(f1)\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(alpha, accuracy, label='accuracy')\n", "plt.plot(alpha, f1score, label='F1 score')\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Simulate the whole 18/19 season"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Given the first five matches, the remaining matches will be simulated by the previous models"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[86]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from numpy.random import lognormal"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SimulateAllSeason:\n", "    def __init__(self, season, team_res_dict):\n", "        self.season = season\n", "        self.team_res_dict = team_res_dict\n", "        \n", "        self.matchs_df = pd.read_csv(f'clean_data/{season}.csv').sort_values('Date', ignore_index=True).drop('Unnamed: 0', axis=1)\n", "        \n", "        self.fixture_df = self.matchs_df[['Date', 'HomeTeam', 'AwayTeam']].copy()\n", "        self.allteams = self.matchs_df['HomeTeam'].unique()\n", "        self.progress_df = pd.DataFrame([], columns=['Date', 'HomeTeam', 'HomeScore', 'AwayScore', 'AwayTeam', 'Winner'])\n", "        \n", "        # all possible regression variables\n", "        self.cols = [\n", "            'Round', 'isHome', \n", "            'b5MatchGoal', \n", "            'bRival5MatchGoal', 'bRival5MatchConceded',\n", "            'SelfAS', 'SelfDS', 'RivalAS', 'RivalDS',\n", "            'SelfFromCL', 'RivalFromCL',\n", "            'PtDiff', 'isDec'\n", "        ]\n", "        \n", "    def _create_table(self):\n", "        \"\"\"create table based on `self.progress_df`\"\"\"\n", "        progress_df = self.progress_df\n", "        table = []\n", "        \n", "        for team in self.allteams:\n", "            idx_home = progress_df['HomeTeam'] == team\n", "            idx_away = progress_df['AwayTeam'] == team\n", "            idx = idx_home | idx_away\n", "            \n", "            winner = progress_df.loc[idx, 'Winner']\n", "            num_win = (winner == team).sum()\n", "            num_draw = (winner == 'Draw').sum()\n", "            num_loss = ((winner != team) & (winner != 'Draw')).sum()\n", "            \n", "            points = num_win * 3 + num_draw * 1\n", "            \n", "            goal = progress_df.loc[idx_home, 'HomeScore'].sum() + progress_df.loc[idx_away, 'AwayScore'].sum()\n", "            conceded = progress_df.loc[idx_home, 'AwayScore'].sum() + progress_df.loc[idx_away, 'HomeScore'].sum()\n", "            gc = f'{goal}:{conceded}'\n", "            \n", "            table.append([team, num_win, num_draw, num_loss, gc, points])\n", "            \n", "        table = pd.DataFrame(table)\n", "        table.columns = ['Team', 'Win', 'Draw', 'Loss', 'Goals', 'Points']\n", "        table = table.sort_values(by='Points', ascending=False, ignore_index=True)\n", "        table.insert(0, 'Rank', list(range(1, 20+1)))\n", "        return table\n", "    \n", "    def _calc_round(self, team):\n", "        idx = (self.progress_df['HomeTeam'] == team) | (self.progress_df['AwayTeam'] == team)\n", "        return idx.sum() + 1     \n", "        \n", "    def _calc_b5MatchGoal(self, team):\n", "        team_at_home = self.progress_df['HomeTeam'] == team\n", "        team_at_away = self.progress_df['AwayTeam'] == team\n", "        team_matchs = team_at_home | team_at_away\n", "        \n", "        team_last5_df = self.progress_df[team_matchs].tail(5).copy()        \n", "        goals = team_last5_df.loc[team_at_home, 'HomeScore'].sum() + team_last5_df.loc[team_at_away, 'AwayScore'].sum()\n", "        return goals\n", "    def _calc_b5MatchConceded(self, team):\n", "        team_at_home = self.progress_df['HomeTeam'] == team\n", "        team_at_away = self.progress_df['AwayTeam'] == team\n", "        team_matchs = team_at_home | team_at_away\n", "        \n", "        team_last5_df = self.progress_df[team_matchs].tail(5).copy()\n", "        conceded = team_last5_df.loc[team_at_home, 'AwayScore'].sum() + team_last5_df.loc[team_at_away, 'HomeScore'].sum()\n", "        return conceded\n", "    \n", "    def _calc_ADC(self, team, rival, is_team_home):\n", "        \"\"\"\n", "        Return: pd.Series, index = [SelfAS, SelfDS, RivalAS, RivalDS, SelfFromCL, RivalFromCL]\n", "        \"\"\"\n", "        team_df = pd.read_csv(f'team_data/{self.season}/{team}.csv')\n", "        idx = (team_df['Rival'] == rival) & (team_df['isHome'] == is_team_home)\n", "        adc = team_df.loc[idx, ['SelfAS', 'SelfDS', 'RivalAS', 'RivalDS', 'SelfFromCL', 'RivalFromCL']].squeeze()\n", "        adc[['SelfFromCL', 'RivalFromCL']] = adc[['SelfFromCL', 'RivalFromCL']].astype(int)\n", "        return adc\n", "    \n", "    def _calc_PtDiff(self, team, rival):\n", "        table = self._create_table()\n", "                \n", "        played_matchs = table[['Win', 'Draw', 'Loss']].apply(lambda row: row.sum(), axis=1).values\n", "        avg_points = table['Points'].values / played_matchs\n", "        points = avg_points * played_matchs.min()\n", "        \n", "        team_idx = table['Team'] == team\n", "        rival_idx = table['Team'] == rival\n", "        std_points_team = (points[team_idx] - points.mean()) / points.std(ddof=1)\n", "        std_points_rival = (points[rival_idx] - points.mean()) / points.std(ddof=1)\n", "        \n", "        ptdiff = (std_points_team - std_points_rival)[0]    # array to scalar\n", "        return ptdiff\n", "    \n", "    def _count_match_played(self, team):\n", "        idx = (self.progress_df['HomeTeam'] == team) | (self.progress_df['AwayTeam'] == team)\n", "        return idx.sum()\n", "    \n", "    def _get_real_result(self, home_team, away_team):\n", "        idx = (self.matchs_df['HomeTeam'] == home_team) & (self.matchs_df['AwayTeam'] == away_team)\n", "        home_goal = self.matchs_df.loc[idx, 'HomeScore'].values[0]   # array to scalar\n", "        away_goal = self.matchs_df.loc[idx, 'AwayScore'].values[0]\n", "        return home_goal, away_goal\n", "    \n", "    def _predict_team_score(self, team, rival, is_team_home, date, method='sample'):\n", "        \"\"\"\n", "        team, rival : str\n", "            name of the team and its rival\n", "        is_team_home : bool\n", "            determine if `team` is played at home\n", "        date : str\n", "            e.g. date = '2020-10-12'\n", "        method : str, 'directly', 'sample' or 'resample'\n", "            Let mu = xi^T * beta, regression model predicted value\n", "                y  = predicted goals\n", "            (1) if 'directly': \n", "                y = floor(mu)\n", "            (2) if 'sample': \n", "                y is sampled from poisson(mu)\n", "            (3) if 'resample':\n", "                mu_new = log-normal(mu, xi^T * Cov(beta) * xi)\n", "                and y is sampled from poisson(mu_new)\n", "        \"\"\"\n", "        # Series, index = [SelfAS, SelfDS, RivalAS, RivalDS, SelfFromCL, RivalFromCL]\n", "        adc = self._calc_ADC(team, rival, is_team_home)       \n", "        \n", "        data = pd.Series({\n", "            'const': 1,\n", "            'Round': self._calc_round(team),\n", "            'isHome': 1 if is_team_home else 0,\n", "            'b5MatchGoal': self._calc_b5MatchGoal(team),\n", "            'bRival5MatchGoal': self._calc_b5MatchGoal(rival),\n", "            'bRival5MatchConceded': self._calc_b5MatchConceded(rival),\n", "            'SelfAS': adc['SelfAS'],\n", "            'SelfDS': adc['SelfDS'],\n", "            'RivalAS': adc['RivalAS'],\n", "            'RivalDS': adc['RivalDS'],\n", "            'SelfFromCL': adc['SelfFromCL'],\n", "            'RivalFromCL': adc['RivalFromCL'],\n", "            'PtDiff': self._calc_PtDiff(team, rival),\n", "            'isDec': 1 if pd.to_datetime(date).month == 12 else 0\n", "        })\n", "               \n", "        team_res = self.team_res_dict[team]\n", "        regdata = data[team_res.model.exog_names]   \n", "        \n", "        mu = team_res.predict(regdata.to_list())[0]   # array to scalar\n", "            \n", "        # something strange happened... set an upper bound to prevent the unreasonable result\n", "        if mu >= 4:\n", "            mu = 4\n", "            \n", "        if method == 'directly':\n", "            predict = int(np.floor(mu))\n", "            \n", "        elif method == 'sample':\n", "            predict = poisson.rvs(mu)\n", "            \n", "        elif method == 'resample':\n", "            # beta_hat ~ N(beta, F^-1)\n", "            # ->  eta_i = x_i^T * beta_hat ~ N(x_i^T * beta, x_i^T * F^-1 * x_i)\n", "            # ->  mu_i = log(eta_i) ~ Log-Normal\n", "            # resample `mu` from log-normal distribution \n", "            # with mean = x_i^T * beta and var = x_i^T * F^-1 * x_i\n", "            xi = regdata    # (p,)\n", "            beta = team_res.params    # (p,)\n", "            F_inv = team_res.cov_params()    # (p, p)\n", "            mean = xi @ beta\n", "            var = xi.T @ F_inv @ xi\n", "            mu = lognormal(mean=mean, sigma=np.sqrt(var))\n", "            if mu >= 4:\n", "                mu = 4\n", "            predict = poisson.rvs(mu)\n", "            \n", "        else:\n", "            raise ValueError(\"unacceptable method\")\n", "            \n", "        return predict\n", "    \n", "    def _predict_scores(self, home_team, away_team, date, method='sample'):\n", "        home_goal = self._predict_team_score(home_team, away_team, True, date, method)\n", "        away_goal = self._predict_team_score(away_team, home_team, False, date, method)\n", "        return home_goal, away_goal\n", "    \n", "    def run(self, method='sample'):\n", "        \"\"\"\n", "        method : str, 'directly', 'sample' or 'resample'\n", "            Let log(mu) = xi^T * beta, the regression model predicted value\n", "            where beta is normal distribution,\n", "            and Let y  = predicted goals\n", "            (1) if 'directly': \n", "                y = floor(mu)\n", "            (2) if 'sample': \n", "                y is sampled from poisson(mu)\n", "            (3) if 'resample':\n", "                mu_new = log-normal(mu, xi^T * Cov(beta) * xi)\n", "                and y is sampled from poisson(mu_new)\n", "        \"\"\"\n", "        nrows = self.fixture_df.shape[0]\n", "        \n", "        for i, row in self.fixture_df.iterrows():\n", "            date = row['Date']\n", "            home_team = row['HomeTeam']\n", "            away_team = row['AwayTeam']\n", "            \n", "            print(f'{i+1:3}/{nrows}     {date} {home_team:>15} vs {away_team:<15}', end='\\r')\n", "            \n", "            if self._count_match_played(home_team) <= 8 or self._count_match_played(away_team) <= 8:\n", "                # use the real match result\n", "                home_goal, away_goal = self._get_real_result(home_team, away_team)\n", "            else:\n", "                home_goal, away_goal = self._predict_scores(home_team, away_team, date, method)\n", "                \n", "            if home_goal > away_goal:\n", "                winner = home_team\n", "            elif home_goal < away_goal:\n", "                winner = away_team\n", "            else:\n", "                winner = 'Draw'\n", "                \n", "            self.progress_df = self.progress_df.append(\n", "                {\n", "                    'Date': date,\n", "                    'HomeTeam': home_team,\n", "                    'HomeScore': home_goal,\n", "                    'AwayScore': away_goal,\n", "                    'AwayTeam': away_team,\n", "                    'Winner': winner\n", "                },\n", "                ignore_index=True\n", "            )\n", "            \n", "        print()\n", "        self.table = self._create_table()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## simulate and display the final table"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[61]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["simulate = SimulateAllSeason('1819', team_res_dict)\n", "simulate.run(method='sample')\n", "simulate.table"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## simulate 200 times"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[82]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def large_simulations(team_res_dict, method, ntimes=200):\n", "    \"\"\"\n", "    return Dict[str, List[List[int]]]\n", "    e.g. {'Arsenal': [[1, 29,  0,  9, 77, 33, 87], [2, 24,  4, 10, 77, 45, 76], ...]}\n", "    each row = [rank, win, draw, loss, goals, conceded, points]\n", "    and there are `ntimes` rows\n", "    \"\"\"\n", "    allteams = list(team_res_dict.keys())\n", "    simu_dict = {team: [] for team in allteams}   # Dict[str_team_name, List_i'th_simulate[List_result[int]]]\n", "    for i in range(ntimes):\n", "        print(f'{i}/{ntimes}')\n", "        simulate = SimulateAllSeason('1819', team_res_dict)\n", "        simulate.run(method=method)\n", "        table = simulate.table\n\n", "        # update `large_simulation`\n", "        for team in allteams:\n", "            idx = table['Team'] == team\n", "            rank = table.loc[idx, 'Rank'].values[0]\n", "            win = table.loc[idx, 'Win'].values[0]\n", "            draw = table.loc[idx, 'Draw'].values[0]\n", "            loss = table.loc[idx, 'Loss'].values[0]\n", "            goals, conceded = table.loc[idx, 'Goals'].str.split(':').values[0]\n", "            goals = int(goals)\n", "            conceded = int(conceded)\n", "            points = table.loc[idx, 'Points'].values[0]\n", "            simu_dict[team].append([rank, win, draw, loss, goals, conceded, points])\n", "            \n", "    return simu_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[83]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sample_simu_dict = large_simulations(team_res_dict, method='sample', ntimes=200)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[87]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["resample_simu_dict = large_simulations(team_res_dict, method='resample', ntimes=200)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[161]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from collections import Counter"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_simu_dict(simu_dict):\n", "    \"\"\"simu_dict = large_simulations(...)\"\"\"\n", "    real_table = pd.read_csv('table/1819.csv')\n", "    allteams = real_table['Team'].values   # team names are sorted by points\n", "    fig, axs = plt.subplots(nrows=5, ncols=4, figsize=(20, 20), sharex=True, sharey=True)\n", "    for ax, team in zip(axs.ravel(), allteams):\n", "        simu_res = np.array(simu_dict[team])\n", "        rank = simu_res[:,0]\n", "        win = simu_res[:,1]\n", "        draw = simu_res[:,2]\n", "        loss = simu_res[:,3]\n", "        goals = simu_res[:,4]\n", "        concede = simu_res[:,5]\n", "        points = simu_res[:,6]\n", "        counter = Counter(rank)\n", "        ax.bar(counter.keys(), counter.values(), width=1, edgecolor='k', alpha=0.6)\n", "        ax.set_title(team, fontsize=18)\n", "        real_rank = real_table.loc[real_table.Team == team, 'Rank'].values[0]\n", "        #ylim = ax.get_ylim()\n", "        #ax.vlines(real_rank, ylim[0], ylim[1], colors='k', ls='dashed', lw=2)\n", "        ax.vlines(real_rank, 0, 200, colors='k', ls='dashed', lw=2)\n", "        ax.set_xlim([0, 21])\n", "        ax.set_xticks(list(range(1, 21)))\n", "        \n", "        ax.set_ylim([0, 110])\n", "    axs[-1,0].set_xlabel('Rank', fontsize=25)\n", "    axs[-1,0].set_ylabel('frequency', fontsize=25)\n", "    plt.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[162]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_simu_dict(sample_simu_dict)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[163]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_simu_dict(resample_simu_dict)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}